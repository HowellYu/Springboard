{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since read_json does not work for such a large dataset, I worked out something out.\n",
    "review_data = {'business_id':[], 'date':[], 'review_id':[], 'stars':[], 'text':[], 'user_id':[]}\n",
    "\n",
    "\n",
    "with open('yelp_academic_dataset_review.json', 'r') as infile:\n",
    "    for line in infile:\n",
    "        row = json.loads(line)\n",
    "        for key in review_data.keys():\n",
    "            if key != '':\n",
    "                review_data[key].append(row[key])\n",
    "            else:\n",
    "                review_data[key].append(\"\")\n",
    "review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save review data as a dataframe \n",
    "review_data = pd.DataFrame(review_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a column to count the number of words for each comment\n",
    "import re\n",
    "text = review_data.text\n",
    "storage = []\n",
    "for i in range(len(text)):\n",
    "    words = re.findall(r\"[\\w']+\", text[i])\n",
    "    storage.append(len(words))\n",
    "review_data[\"review_len\"] = pd.Series(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a column to show the month of the review\n",
    "import re \n",
    "date = review_data.date\n",
    "date_drop_day = []\n",
    "for i in range(len(date)):\n",
    "    date_drop_day.append(int(date[i][:-3].replace(\"-\",\"\")))\n",
    "review_data[\"date_drop_day\"] = pd.Series(date_drop_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Dataset with Unique `business_id`\n",
    "business_id, date_drop_day, stars, text, all reviews, number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unique business id\n",
    "unique_bus_id = review_data.business_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_review_data = review_data\n",
    "new_review_data.drop('date', axis=1, inplace=True)\n",
    "new_review_data.drop('review_id', axis=1, inplace=True)\n",
    "new_review_data.drop('user_id', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main \n",
    "Assumptions of Fake Reviews: \n",
    "1.\tSince the purpose of fake reviews is to increase the both the star ratings and quality of comments, during the month where fake reviewers are hired, there will be a significant increase in star rating.Â \n",
    "2.\tSince fake reviewers may comments based on certain templates and for the same fake reviewer, he or she might have given out fake reviewers for a number of different business, it is highly likely that businesses which hired fake reviewers will have highly similar reviews.\n",
    "3.\tOnly a very small portion of businesses hired fake reviewers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Filter Possible Fake Reviews Based on Change of Star Ratings and Number of Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unique_review_count record the count of each business id\n",
    "unique_review_count = pd.DataFrame({'count' : new_review_data.groupby( [ \"business_id\"] ).size()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# groupby both business id and date\n",
    "unique_review_count2 = pd.DataFrame({'count' : new_review_data.groupby(\n",
    "    [ \"business_id\", \"date_drop_day\"]).size()}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for the same business id, filter out suspects who has a huge difference between consecutive months\n",
    "suspects = []\n",
    "unique_review_count3 = unique_review_count2\n",
    "unique_review_count3.drop('date_drop_day', axis=1, inplace=True)\n",
    "\n",
    "for i in range(len(unique_review_count3.business_id)-1):\n",
    "    current_index = unique_review_count3.loc[i] \n",
    "    next_index = unique_review_count3.loc[i+1]\n",
    "    if current_index['business_id'] == next_index['business_id'] and current_index['business_id'] not in suspects:\n",
    "        if current_index['count'] > 30:\n",
    "            if current_index['count'] * 1.5 <= next_index['count']:\n",
    "                suspects.append(current_index['business_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find if There Is Very Similar Text Between Different Restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter new_review_data with business id in suspects\n",
    "suspect_df = new_review_data[new_review_data.business_id.isin(suspects)]\n",
    "suspect_df = suspect_df.reset_index()\n",
    "del suspect_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert Series into list\n",
    "text = suspect_df.text\n",
    "text = text.tolist()\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# initialize\n",
    "vectorizer = CountVectorizer(min_df=0.001)\n",
    "vectorizer.fit(text)\n",
    "x = vectorizer.transform(text)\n",
    "x = x.toarray()\n",
    "\n",
    "print(\"Transformed text vector is \\n{}\".format(x))\n",
    "print(\"\")\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test \n",
    "temp_suspects = [suspects[0], suspects[1]]\n",
    "temp_suspects\n",
    "\n",
    "storage = []\n",
    "exec(open('suspects.txt').read())\n",
    "for id1 in temp_suspects:\n",
    "    temp_suspects.remove(id1)\n",
    "    print(len(temp_suspects))\n",
    "    for id2 in temp_suspects:\n",
    "        index1 = list(suspect_df.text[suspect_df.business_id == id1].index)\n",
    "        index2 = list(suspect_df.text[suspect_df.business_id == id2].index)\n",
    "        vector_storage1 = x[index1]\n",
    "        vector_storage2 = x[index2]\n",
    "        temp_storage = []\n",
    "        \n",
    "        for vec1 in vector_storage1:\n",
    "            for vec2 in vector_storage2: \n",
    "                 temp_storage.append(1 - spatial.distance.cosine(vec1, vec2))\n",
    "        storage.append(temp_storage)\n",
    "storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_score = pd.DataFrame(index=suspects, columns=suspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the cosine between comments under different business \n",
    "exec(open('suspects.txt').read())\n",
    "for id1 in suspects:\n",
    "    suspects.remove(id1)\n",
    "    print(len(suspects))\n",
    "    for id2 in suspects:\n",
    "        index1 = list(suspect_df.text[suspect_df.business_id == id1].index)\n",
    "        index2 = list(suspect_df.text[suspect_df.business_id == id2].index)\n",
    "        vector_storage1 = x[index1]\n",
    "        vector_storage2 = x[index2]\n",
    "        temp_storage = []\n",
    "        \n",
    "        for vec1 in vector_storage1:\n",
    "            for vec2 in vector_storage2: \n",
    "                 temp_storage.append(1 - spatial.distance.cosine(vec1, vec2))\n",
    "        cos_score.loc[id1, id2] = temp_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cluster Comments \n",
    "If we directly compute the cosine, it is really computationally expensive. Thus, if we assume similar comment will be clustered into the same group, we can reduce the computation cost by clustering the comments first and then compute and compare the cosine between 2 comments of different `business_id` of each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=20)\n",
    "label = cluster.fit_predict(x)\n",
    "df_pivot = copy.deepcopy(suspect_df)\n",
    "df_pivot['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "x = np.load('x_matrix.txt.npy')\n",
    "df_200_storage = pd.DataFrame(columns=['business1', 'business2', 'group', 'angle'])\n",
    "for i in range(200):\n",
    "    print('---->', i)\n",
    "    temp_suspects = list(df_pivot_200[df_pivot_200.label == i].business_id.unique())\n",
    "    print('suspect length', len(temp_suspects))\n",
    "    df_pivot_temp = df_pivot_200[df_pivot_200.label == i]\n",
    "    print('total length of this group:', len(df_pivot_temp))\n",
    "    \n",
    "    for id1 in temp_suspects:\n",
    "        temp_suspects.remove(id1)\n",
    "        print('suspects in the current group', len(temp_suspects))\n",
    "        counter = 0\n",
    "\n",
    "        if k % 2000 == 0: \n",
    "            print('Saving k at k =', k)\n",
    "            df_200_storage.to_csv('df_200_storage.csv')\n",
    "\n",
    "        for id2 in temp_suspects:\n",
    "            print(counter)\n",
    "            counter = counter + 1\n",
    "\n",
    "            index1 = list(df_pivot_temp.text[df_pivot_temp.business_id == id1].index)\n",
    "            index2 = list(df_pivot_temp.text[df_pivot_temp.business_id == id2].index)\n",
    "            vector_storage1 = x[index1]\n",
    "            vector_storage2 = x[index2]\n",
    "\n",
    "            for vec1 in vector_storage1:\n",
    "                for vec2 in vector_storage2: \n",
    "                    df_200_storage.loc[k] = [id1, id2, i, 1 - spatial.distance.cosine(vec1, vec2)]\n",
    "                    k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(df_200_storage.angle, bins = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.boxplot(df_200_storage.angle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_200_storage.angle.describe() # third quantile is 0.744664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use third quantile as threshold\n",
    "final_suspects_detail = df_200_storage[df_200_storage.angle >= 0.744664]\n",
    "final_suspects_detail.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_suspects_detail.to_csv('final_suspects_detail.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
